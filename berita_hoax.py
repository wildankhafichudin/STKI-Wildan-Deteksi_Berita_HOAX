import streamlit as st

# -*- coding: utf-8 -*-
"""DETEKSI BERITA HOAX

Berita Hoax2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kNbbguxUhDgyjQHujVAI3Fx70Fi92B9Z
"""

import pandas as pd

dummy_doc = pd.DataFrame({"text":["utk kamu yg disana, sdh makan belum?",
                                  "Bangkai 120 lumba-lumba ditemukan di sungai Amazon selama seminggu terakhir.",
                                  "Gue ga suka makan bakso, jangan dipaksaaa!!!",
                                  "Galau bgt mau dukung Ji Chang Wook atau Wi Ha Joon... Semuanya kasian huhu.. The Worst of Evil seruuuu bgt! <3",
                                  "Kasus Covid-19 bertambah 40.618 pada Kamis (10/2), sehingga total kasus mencapai 4.667.554. Sebanyak 4.234.510 orang telah sembuh, dan 144.858 meninggal dunia."
                                  ]})
dummy_doc

for row in range(0,5):
  print(dummy_doc.iloc[row,0])

def print_text_in_df(doc):
  for row in range(0,doc.shape[0]):
    print(doc.iloc[row,0])

def word_token_default(doc):
  return doc.split()

print(word_token_default(dummy_doc.iloc[3,0]))

dummy_doc1 = dummy_doc.apply(lambda doc: word_token_default(doc.iloc[0]),axis=1)
print(type(dummy_doc1))

#Menampilkan kata
print_text_in_df(dummy_doc1.to_frame(name="text"))

from spacy.lang.id import Indonesian
import spacy
nlp = Indonesian()

def word_token_spacy(doc):
  doc1 = nlp(doc)
  token = [token.text for token in doc1]
  return token

print(word_token_spacy(dummy_doc.iloc[3,0]))

dummy_doc2 = dummy_doc.apply(lambda doc: word_token_spacy(doc.iloc[0]),axis=1)
print(type(dummy_doc2))
dummy_doc2

print_text_in_df(dummy_doc2.to_frame(name="text"))

eng_text = "I don't like your attitude! I'll report you to your parents!"
print(word_token_default(eng_text))
print(word_token_spacy(eng_text))

nlp_eng = spacy.load("en_core_web_sm")
def word_token_spacy_eng(doc):
  doc1 = nlp_eng(doc)
  token = [token.text for token in doc1]
  return token

print(word_token_spacy_eng(eng_text))

import nltk
from nltk.tokenize import word_tokenize

nltk.download('all')

print(word_tokenize(dummy_doc.iloc[3,0]))

print(word_tokenize(eng_text))

dummy_doc3 = dummy_doc.apply(lambda doc: word_tokenize(doc.iloc[0]),axis=1)
print(type(dummy_doc3))
dummy_doc3

print_text_in_df(dummy_doc3.to_frame(name="text"))

def char_token(doc):
 token = [x for x in doc]
 return token

print(char_token(dummy_doc.iloc[3,0]))

dummy_doc4 = dummy_doc.apply(lambda doc: char_token(doc.iloc[0]),axis=1)
print(type(dummy_doc4))
dummy_doc4

print_text_in_df(dummy_doc4.to_frame(name="text"))

#pip install bpemb

from bpemb import BPEmb

bpemb_id = BPEmb(lang="id",vs=200000, dim=300)

print(bpemb_id.encode(dummy_doc.iloc[3,0]))

dummy_doc5 = dummy_doc.apply(lambda doc: bpemb_id.encode(doc.iloc[0]),axis=1)
print(type(dummy_doc5))
dummy_doc5

print_text_in_df(dummy_doc5.to_frame(name="text"))

import string
print(string.punctuation)
print(string.digits)

def pre_token(doc):
  # case folding
  doc1 = doc.lower()
  # punctuation removal+menghapus angka
  doc2 = doc1.translate(str.maketrans('', '', string.punctuation + string.digits))
  # whitespace removal
  doc3 = doc2.strip()
  return doc3

def pre_token_print(doc):
  # case folding
  doc1 = doc.lower()
  print("Tahap Case Folding \n")
  print(doc1,"\n")
  # punctuation removal+menghapus angka
  doc2 = doc1.translate(str.maketrans('', '', string.punctuation + string.digits))
  print("Tahap Punctuation Removal dan menghapus angka \n")
  print(doc2,"\n")
  # whitespace removal
  print("Tahap Whitespace Removal \n")
  doc3 = doc2.strip()
  print(doc3)

print("Sebelum Preprocessing \n")
print(dummy_doc.iloc[3,0])
print("\nSesudah Preprocessing \n")
pre_token(dummy_doc.iloc[3,0])

pre_token_print(dummy_doc.iloc[3,0])

dummy_doc6 = dummy_doc.apply(lambda doc: pre_token(doc.iloc[0]),axis=1)
print(type(dummy_doc6))
dummy_doc6

print("Sebelum Preprocessing \n")
print_text_in_df(dummy_doc)
print("\nSesudah Preprocessing \n")
print_text_in_df(dummy_doc6.to_frame(name="text"))

#fungsi menghapus stopword
def stopwords_removal(words,stopword):
    return [word for word in words if word not in stopword]

from spacy.lang.id.stop_words import STOP_WORDS

print(type(STOP_WORDS))
len(STOP_WORDS)

print(list(STOP_WORDS)[0:20])

def find_word(word,doc):
  return list(filter(lambda x: word in x, doc))

find_word("tidak",list(STOP_WORDS))

print("Sebelum Preprocessing \n")
print(dummy_doc.iloc[3,0])
print("Sesudah Preprocessing \n")
print(stopwords_removal(word_token_spacy(dummy_doc.iloc[3,0]),STOP_WORDS))

print(stopwords_removal(dummy_doc.iloc[3,0],STOP_WORDS))

dummy_doc7 = dummy_doc.apply(lambda doc: stopwords_removal(word_token_spacy(doc.iloc[0]),STOP_WORDS),axis=1)
print(type(dummy_doc7))
dummy_doc7

print("Sebelum Preprocessing \n")
print_text_in_df(dummy_doc)
print("\nSesudah Preprocessing \n")
print_text_in_df(dummy_doc7.to_frame(name="text"))

from nltk.corpus import stopwords

# mendapatkan stopword bahasa indonesia
indo_stopwords = stopwords.words('indonesian')
print(type(indo_stopwords))
len(indo_stopwords)

print(indo_stopwords[0:20])

find_word("tidak",indo_stopwords)

print("Sebelum Preprocessing \n")
print(dummy_doc.iloc[3,0])
print("\nSesudah Preprocessing \n")
print(stopwords_removal(word_tokenize(dummy_doc.iloc[3,0]),indo_stopwords))

print(stopwords_removal(dummy_doc.iloc[3,0],indo_stopwords))

dummy_doc8 = dummy_doc.apply(lambda doc: stopwords_removal(word_tokenize(doc.iloc[0]),indo_stopwords),axis=1)
print(type(dummy_doc8))
dummy_doc8

print("Sebelum Preprocessing \n")
print_text_in_df(dummy_doc)
print("\nSesudah Preprocessing \n")
print_text_in_df(dummy_doc8.to_frame(name="text"))

#pip install PySastrawi

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
factory = StopWordRemoverFactory()
stopword_sastrawi = factory.get_stop_words()
print(stopword_sastrawi)

print("Sebelum Preprocessing \n")
print(dummy_doc.iloc[1,0])
print("\nSesudah Preprocessing \n")
print(stopwords_removal(word_tokenize(dummy_doc.iloc[1,0]),stopword_sastrawi))

import pandas as pd

indo_slang_word = pd.read_csv("https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv")
indo_slang_word.head()

indo_slang_word.query("slang == 'loe'")

def replace_slang_word(doc,slang_word):
    for index in  range(0,len(doc)-1):
        index_slang = slang_word.slang==doc[index]
        formal = list(set(slang_word[index_slang].formal))
        if len(formal)==1:
            doc[index]=formal[0]
    return doc

print("Sebelum Preprocessing \n")
print(dummy_doc.iloc[2,0])
print("\nSesudah Preprocessing \n")
print(replace_slang_word(word_token_spacy(dummy_doc.iloc[2,0]),indo_slang_word))

dummy_doc9 = dummy_doc.apply(lambda doc: replace_slang_word(word_token_spacy(doc.iloc[0]),indo_slang_word),axis=1)
print(type(dummy_doc9))
dummy_doc9

print("Sebelum Preprocessing \n")
print_text_in_df(dummy_doc)
print("\nSesudah Preprocessing \n")
print_text_in_df(dummy_doc9.to_frame(name="text"))

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()

kalimat = "Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah."
hasil = stemmer.stem(kalimat)
print(hasil)

print("Sebelum Preprocessing \n")
print(dummy_doc.iloc[4,0])
print("\nSesudah Preprocessing \n")
print(stemmer.stem(dummy_doc.iloc[4,0]))

# Porter Stemmer
from nltk.stem import PorterStemmer
ps = PorterStemmer()

ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')

ps.stem('lying'), ps.stem('strange')

# Lancaster Stemmer
from nltk.stem import LancasterStemmer
ls = LancasterStemmer()

ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')

ls.stem('lying'), ls.stem('strange')

# Regex based stemmer
from nltk.stem import RegexpStemmer
rs = RegexpStemmer('ing$|s$|ed$', min=4)
rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped')

rs.stem('lying'), rs.stem('strange')

# Snowball Stemmer
from nltk.stem import SnowballStemmer
ss = SnowballStemmer("german")
print('Supported Languages:', SnowballStemmer.languages)

# stemming on German words
# autobahnen -> cars
# autobahn -> car
ss.stem('autobahnen')

# springen -> jumping
# spring -> jump
ss.stem('springen')

def simple_stemmer(text):
    ps = nltk.porter.PorterStemmer()
    text = ' '.join([ps.stem(word) for word in text.split()])
    return text

simple_stemmer("My system keeps crashing his crashed yesterday, ours crashes daily")

import nltk
nltk.download('punkt')

#pip install nlp-id

import pandas as pd
import string
import re
import json
from nlp_id.tokenizer import Tokenizer
from nlp_id.stopword import StopWord
from nlp_id.lemmatizer import Lemmatizer
from sklearn.feature_extraction.text import CountVectorizer

berita_hoax = pd.read_csv('berita_HOAX_indonesia.csv', delimiter=";")
berita_hoax.head()

berita_hoax['kategori'].value_counts()

with open('combined_slang_words.txt') as f:
    data0 = f.read()
print("Data type before reconstruction : ", type(data0))
formal_indo = json.loads(data0)
print("Data type after reconstruction : ", type(formal_indo))

def informal_to_formal_indo(text):
    res = " ".join(formal_indo.get(ele, ele) for ele in text.split())
    return(res)

tokenizer = Tokenizer()
stopword = StopWord()
lemmatizer = Lemmatizer()

lemmatizer.lemmatize("gue mau berpulang")

def my_tokenizer(doc):
  #menghapus url
  doc1 = re.sub(r"https\S+", "",doc)
  # lowercase
  doc3 = doc1.lower()
  # Text Normalization
  doc4 = informal_to_formal_indo(doc3)
  # punctuation removal+menghapus angka
  doc5 = doc4.translate(str.maketrans('', '', string.punctuation + string.digits))
  # whitespace removal
  doc6 = doc5.strip()
  # stopword removal
  doc7 = stopword.remove_stopword(doc6)
  #Lemmatization
  doc8=lemmatizer.lemmatize(doc7)
  # tokenization
  doc_token1 = tokenizer.tokenize(doc8)
  return doc_token1

berita_hoax.iloc[1,1]

count_vect0 = CountVectorizer(tokenizer=my_tokenizer,token_pattern=None,lowercase=False)

features = count_vect0.fit_transform(berita_hoax["berita"])
features_df = pd.DataFrame(features.todense(), columns=[f"feature_{i}" for i in range(features.shape[1])])
combined_df = pd.concat([berita_hoax["kategori"], features_df], axis=1)
combined_df

combined_df.to_csv("berita_hoax_indonesia_postprocess.csv", index=False)

from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()

# lemmatize nouns
print(wnl.lemmatize('cars', 'n'))
print(wnl.lemmatize('men', 'n'))

# lemmatize verbs
print(wnl.lemmatize('running', 'v'))
print(wnl.lemmatize('ate', 'v'))

# lemmatize adjectives
print(wnl.lemmatize('saddest', 'a'))
print(wnl.lemmatize('fancier', 'a'))

# ineffective lemmatization
print(wnl.lemmatize('ate', 'n'))
print(wnl.lemmatize('fancier', 'v'))

import spacy
# use spacy.load('en') if you have downloaded the language model en directly after install spacy
nlp = spacy.load('en_core_web_sm')
text = 'My system keeps crashing his crashed yesterday, ours crashes daily'

def lemmatize_text(text):
    text = nlp(text)
    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
    return text

lemmatize_text("My system keeps crashing! his crashed yesterday, ours crashes daily")

from nltk.tokenize.toktok import ToktokTokenizer
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):
    tokens = tokenizer.tokenize(text)
    tokens = [token.strip() for token in tokens]
    if is_lower_case:
        filtered_tokens = [token for token in tokens if token not in stopwords]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text

remove_stopwords("The, and, if are stopwords, computer is not")

import re
from bs4 import BeautifulSoup

def strip_html_tags(text):
    soup = BeautifulSoup(text, "html.parser")
    [s.extract() for s in soup(['iframe', 'script'])]
    stripped_text = soup.get_text()
    stripped_text = re.sub(r'[\r|\n|\r\n]+', '\n', stripped_text)
    return stripped_text

import unicodedata

def remove_accented_chars(text):
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return text

remove_accented_chars('Sómě Áccěntěd těxt')

#pip install contractions

import contractions
contractions.fix("Y'all can't expand contractions I'd think")

def remove_special_characters(text, remove_digits=False):
    pattern = r'[^a-zA-z0-9\s]' if not remove_digits else r'[^a-zA-z\s]'
    text = re.sub(pattern, '', text)
    return text

remove_special_characters("Well this was fun! What do you think? 123#@!",
                          remove_digits=True)

old_word = 'finalllyyy'
repeat_pattern = re.compile(r'(\w*)(\w)\2(\w*)')
match_substitution = r'\1\2\3'
step = 1

while True:
    # remove one repeated character
    new_word = repeat_pattern.sub(match_substitution,
                                  old_word)
    if new_word != old_word:
         print('Step: {} Word: {}'.format(step, new_word))
         step += 1 # update step
         # update old word to last substituted state
         old_word = new_word
         continue
    else:
         print("Final word:", new_word)
         break

import nltk
import subprocess
import os

# Download and unzip wordnet
try:
    nltk.data.find('wordnet')
except LookupError:
    nltk.download('wordnet', download_dir='/kaggle/working/')

    # Unzip on Unix-like systems
    if os.name == 'posix':
        command = "unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora"
        subprocess.run(command.split())
    
    nltk.data.path.append('/kaggle/working/')

# Now you can import the NLTK resources as usual
from nltk.corpus import wordnet

from nltk.corpus import wordnet
old_word = 'finalllyyy'
repeat_pattern = re.compile(r'(\w*)(\w)\2(\w*)')
match_substitution = r'\1\2\3'
step = 1

while True:
    # check for semantically correct word
    if wordnet.synsets(old_word):
        print("Final correct word:", old_word)
        break
    # remove one repeated character
    new_word = repeat_pattern.sub(match_substitution,
                                  old_word)
    if new_word != old_word:
        print('Step: {} Word: {}'.format(step, new_word))
        step += 1 # update step
        # update old word to last substituted state
        old_word = new_word
        continue
    else:
        print("Final word:", new_word)
        break

from nltk.corpus import wordnet

def remove_repeated_characters(tokens):
    repeat_pattern = re.compile(r'(\w*)(\w)\2(\w*)')
    match_substitution = r'\1\2\3'
    def replace(old_word):
        if wordnet.synsets(old_word):
            return old_word
        new_word = repeat_pattern.sub(match_substitution, old_word)
        return replace(new_word) if new_word != old_word else new_word

    correct_tokens = [replace(word) for word in tokens]
    return correct_tokens

sample_sentence = 'My schooool is realllllyyy amaaazingggg'
correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))
' '.join(correct_tokens)

import re, collections

def tokens(text):
    """
    Get all words from the corpus
    """
    return re.findall('[a-z]+', text.lower())

WORDS = tokens(open('big.txt').read())
WORD_COUNTS = collections.Counter(WORDS)
# top 10 words in corpus
WORD_COUNTS.most_common(10)

def edits0(word):
    """
    Return all strings that are zero edits away
    from the input word (i.e., the word itself).
    """
    return {word}

def edits1(word):
    """
    Return all strings that are one edit away
    from the input word.
    """
    alphabet = 'abcdefghijklmnopqrstuvwxyz'
    def splits(word):
        """
        Return a list of all possible (first, rest) pairs
        that the input word is made of.
        """
        return [(word[:i], word[i:])
                for i in range(len(word)+1)]

    pairs      = splits(word)
    deletes    = [a+b[1:]           for (a, b) in pairs if b]
    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]
    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]
    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]
    return set(deletes + transposes + replaces + inserts)


def edits2(word):
    """Return all strings that are two edits away
    from the input word.
    """
    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}

def known(words):
    """
    Return the subset of words that are actually
    in our WORD_COUNTS dictionary.
    """
    return {w for w in words if w in WORD_COUNTS}

# input word
word = 'fianlly'

# zero edit distance from input word
edits0(word)

# returns null set since it is not a valid word
known(edits0(word))

# one edit distance from input word
edits1(word)

# get correct words from above set
known(edits1(word))

# two edit distances from input word
edits2(word)

# get correct words from above set
known(edits1(word))

# two edit distances from input word
edits2(word)

# get correct words from above set
known(edits2(word))

candidates = (known(edits0(word)) or
              known(edits1(word)) or
              known(edits2(word)) or
              [word])
candidates

def correct(word):
    """
    Get the best correct spelling for the input word
    """
    # Priority is for edit distance 0, then 1, then 2
    # else defaults to the input word itself.
    candidates = (known(edits0(word)) or
                  known(edits1(word)) or
                  known(edits2(word)) or
                  [word])
    return max(candidates, key=WORD_COUNTS.get)

corrected_word = correct('fianlly')
print(corrected_word)

corrected_word_upper = correct('FIANLLY')
print(corrected_word_upper)

def correct_match(match):
    """
    Spell-correct word in match,
    and preserve proper upper/lower/title case.
    """

    word = match.group()
    def case_of(text):
        """
        Return the case-function appropriate
        for text: upper, lower, title, or just str.:
            """
        return (str.upper if text.isupper() else
                str.lower if text.islower() else
                str.title if text.istitle() else
                str)
    return case_of(word)(correct(word.lower()))


def correct_text_generic(text):
    """
    Correct all the words within a text,
    returning the corrected text.
    """
    return re.sub('[a-zA-Z]+', correct_match, text)

correct_text_generic('fianlly')

correct_text_generic('FIANLLY')

from textblob import Word

w = Word('fianlly')
w.correct()

w.spellcheck()

w = Word('flaot')
w.spellcheck()

def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,
                     accented_char_removal=True, text_lower_case=True,
                     text_lemmatization=True, special_char_removal=True,
                     stopword_removal=True, remove_digits=True):

    normalized_corpus = []
    # normalize each document in the corpus
    for doc in corpus:
        # strip HTML
        if html_stripping:
            doc = strip_html_tags(doc)
        # remove accented characters
        if accented_char_removal:
            doc = remove_accented_chars(doc)
        # expand contractions
        if contraction_expansion:
            doc = contractions.fix(doc)
        # lowercase the text
        if text_lower_case:
            doc = doc.lower()
        # remove extra newlines
        doc = re.sub(r'[\r|\n|\r\n]+', ' ',doc)
        # lemmatize text
        if text_lemmatization:
            doc = lemmatize_text(doc)
        # remove special characters and\or digits
        if special_char_removal:
            # insert spaces between special characters to isolate them
            special_char_pattern = re.compile(r'([{.(-)!}])')
            doc = special_char_pattern.sub(" \\1 ", doc)
            doc = remove_special_characters(doc, remove_digits=remove_digits)
        # remove extra whitespace
        doc = re.sub(' +', ' ', doc)
        # remove stopwords
        if stopword_removal:
            doc = remove_stopwords(doc, is_lower_case=text_lower_case)

        normalized_corpus.append(doc)

    return normalized_corpus

sample_text = ("US unveils world's most powerful supercomputer, beats China. "
               "The US has unveiled the world's most powerful supercomputer called 'Summit', "
               "beating the previous record-holder China's Sunway TaihuLight. With a peak performance "
               "of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, "
               "which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, "
               "which reportedly take up the size of two tennis courts.")

{'Original': sample_text,
 'Processed': normalize_corpus([sample_text])[0]}